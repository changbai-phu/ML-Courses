{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classfication\n",
    "- to predict which categories an example belongs to. \n",
    "- convert a logistic regression model to a binary classification model to predict one of two classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholds and the confusion matrix\n",
    "- **Thresholds**: logistic will give values between 0~1, which is the probability of (for instance) an email is spam or not. We need a threshold value to determine that above what probability, the email is spam. And below what probability, the email is not spam. This is threshold.\n",
    "    - While, considering a corner but also important case, what if the predicted probability equals to the threshold? \n",
    "        - it actually depends on how the tools/platforms/library implement the threshold.\n",
    "            - keras treats the value equals to threshold as negative, which is not spam.\n",
    "    - threshold has to be considered carefully, especially the cost of wrong prediction.\n",
    "        - e.g., face recognition key: cost of recognizing a stranger as a family member and let he/she unlock the key vs cost of recognizing a family member as a stranger.\n",
    "    - threshold also has to deal with **imbalanced dataset**\n",
    "        - e.g., face recognition dataset: maybe 5 out of 50 is family member, and the others are delivery people/technicians/etc\n",
    "- **confusion matrix**: use to track true postive, false positive, false negative and true negative. Actual positive and negative as two columns, predicted positive and negative as two rows. \n",
    "    - true positive: both actual and predicted are positive\n",
    "    - true negative: both actual and predicted are negative\n",
    "    - false positive: actual negative, but predicted positive\n",
    "    - false negative: actual positive, but predicted negative\n",
    "- in general, when the classification threshold increases, both true and false positives decrease, both true and false negtives increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy, recall, precision, and related metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Metric|Guidance|\n",
    "|---|---|\n",
    "|Accuracy|- Use as a rough indicator of model training progress/convergence for balanced datasets.  - For model performance, use only in combination with other metrics. - Avoid for imbalanced datasets. Consider using another metric. |\n",
    "|Recall (True positive rate)|Use when false negatives are more expensive than false positives.|\n",
    "|False positive rate|Use when false positives are more expensive than false negatives.|\n",
    "|Precision|Use when it's very important for positive predictions to be accurate.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = $\\frac{correct\\,classifications}{total\\,classifications}$ = $\\frac{TP+TN}{TP+TN+FP+FN}$\n",
    "\n",
    "Recall (TPR/probability of detection) = $\\frac{correct\\,+ve}{total\\,actual\\,+ve}$ = $\\frac{TP}{TP+FN}$\n",
    "\n",
    "FPR (probability of false alarm) = $\\frac{incorrect\\,classified -ve}{total\\,actual\\,-ve}$ = **$\\frac{FP}{TN+FP}$**\n",
    "\n",
    "Precision = $\\frac{correct\\,classified\\,actual\\,+ve}{total\\,classified\\,+ve}$ = $\\frac{TP}{TP+FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Increasing classification threshold, tends to decrease FP and increase FN.\n",
    "    - So, Precision and Recall always show an inverse relationship. Increasing threshold, Precision improves (closer to 1), but Recall will be getting worsen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC and AUC\n",
    "- **ROC**: Receiver-operating characteristic curve - model performance across all threshold.\n",
    "    - usually x-axis: FPR, y-axis: TPR\n",
    "    - ideal: (0,1) or AUC = 1\n",
    "    - for **AUC = 1**, the classifier will assign an +ve example higher probability being +ve than -ve. \n",
    "- **AUC**: area under ROC curve - representing the probability of correctly ranking random positive and negative examples. In other words, AUC is closer to 1, the better the performance.\n",
    "- For imbalanced dataset, Precision(y) over Recall(x) will be desired for ROC."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
