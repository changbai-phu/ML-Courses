{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "- relationship between features and labels\n",
    "- equation: $y =  b + w_1x_1$, y is the predicted values (output), b is bias (y-intecept and sometimes refer as $w_0$), $w_1$ is weight (slope m), and $x_1$ is the features (input).\n",
    "    - bias and weight are calculated/updated during training.\n",
    "\n",
    "Loss:\n",
    "- the difference between the predicted values and actual values, don't care about the direction -> taking absolute value of the difference or square of the difference.\n",
    "- mainly 4 types of loss:\n",
    "    - $L_1$ loss: $\\sum|actual\\,value - predicted\\,value|$\n",
    "    - MAE (mean absolute error): $\\frac{1}{N}\\sum|actual\\,value - predicted\\,value|$\n",
    "    - $L_2$ loss: $\\sum(actual\\,value - predicted\\,value)^2$\n",
    "    - MSE (mean squared error): $\\frac{1}{N}\\sum(actual\\,value - predicted\\,value)^2$\n",
    "- MSE and MAE are preferred for multiple features\n",
    "- choose proper loss function:\n",
    "    - MSE/$L_2$ loss if want to fit tighly to data, including outliers because squaring will amplify differences (large error, loss larger) and causing high penalty and so the model tends to move heavily toward the outlier.\n",
    "    - MAE/$L_1$ loss if want to avoid outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent:\n",
    "- an iterative method that is used to find the optimal values for parameters (weights and bias) that produce the lowest loss.\n",
    "- steps:\n",
    "    1. Set weight = 0, bias = 0\n",
    "    2. Calculate loss using current paramters\n",
    "    3. Determine the direction to move the weights and bias that reduce loss\n",
    "        - direction: calculate the slope of the tangent to the loss function at each weight and bias = the derivative of the loss function w.r.t the weight and the bias.\n",
    "    4. Move the weight and bias values a small amount (which is gradient multiply by the learning rate) in the direction determined above\n",
    "    5. Repeat step 2 and so on, until the model **converges**.\n",
    "- When graph the loss surface for a model with one feature, it is a **convex** shape (weight and bias have a slop ~ 0).\n",
    "    - A linear model converges when it's found the minimum loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters: \n",
    "- control different aspect of training\n",
    "- 3 common hyperparameters:\n",
    "    - Learning rate: how quickly the model converges\n",
    "        - small -> converge slowly, too many iterations\n",
    "        - large -> never converge, fluctuate\n",
    "    - Batch size: number of examples the model processes before updating weights and bias\n",
    "        - **Stochastic gradient descent (SGD)**\n",
    "            - use only a single example (batch size = 1) per iteration\n",
    "            - produce noise: varations during training cause the loss to increase during iteration \n",
    "        - **Mini-batch stochastic gradient descent**\n",
    "            - 1 < batch size < N \n",
    "            - size choosed at random, take average of gradients, and update weights and bias once per iteration\n",
    "        - larger batch sizes can help reduce the negative effects of having outliers in the data\n",
    "    - Epochs\n",
    "        - means the model has processed every example in the training set once\n",
    "        - e.g., 1000 examples with mini-batch size = 100, it will take 10 iterations to complete one epoch\n",
    "        - more epoch, better model, more time to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions related batch size raised during experiments:\n",
    "- Why Does Small Batch Size Take Longer to Execute? see https://github.com/changbai-phu/ML-Courses/blob/main/Google_ML_Crash_Course/ML_models/Batch_Size_clarification.md "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming exercises\n",
    "- See https://github.com/changbai-phu/ML-Courses/blob/main/ml/cc/exercises/linear_regression_taxi.ipynb for the original files and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title Code - Load dependencies\n",
    "\n",
    "#general\n",
    "import io\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# machine learning\n",
    "import keras\n",
    "\n",
    "# data visualization\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chicago_taxi_dataset = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/chicago_taxi_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title Code - Read dataset\n",
    "\n",
    "# Updates dataframe to use specific columns.\n",
    "training_df = chicago_taxi_dataset[['TRIP_MILES', 'TRIP_SECONDS', 'FARE', 'COMPANY', 'PAYMENT_TYPE', 'TIP_RATE']]\n",
    "\n",
    "print('Read dataset completed successfully.')\n",
    "print('Total number of rows: {0}\\n\\n'.format(len(training_df.index)))\n",
    "training_df.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P2: Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title Code - View dataset statistics\n",
    "\n",
    "print('Total number of rows: {0}\\n\\n'.format(len(training_df.index)))\n",
    "training_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Total number of rows: 31694\n",
    "\n",
    "TRIP_MILES\tTRIP_SECONDS\tFARE\tCOMPANY\tPAYMENT_TYPE\tTIP_RATE\n",
    "count\t31694.000000\t31694.000000\t31694.000000\t31694\t31694\t31694.000000\n",
    "unique\tNaN\tNaN\tNaN\t31\t7\tNaN\n",
    "top\tNaN\tNaN\tNaN\tFlash Cab\tCredit Card\tNaN\n",
    "freq\tNaN\tNaN\tNaN\t7887\t14142\tNaN\n",
    "mean\t8.289463\t1319.796397\t23.905210\tNaN\tNaN\t12.965785\n",
    "std\t7.265672\t928.932873\t16.970022\tNaN\tNaN\t15.517765\n",
    "min\t0.500000\t60.000000\t3.250000\tNaN\tNaN\t0.000000\n",
    "25%\t1.720000\t548.000000\t9.000000\tNaN\tNaN\t0.000000\n",
    "50%\t5.920000\t1081.000000\t18.750000\tNaN\tNaN\t12.200000\n",
    "75%\t14.500000\t1888.000000\t38.750000\tNaN\tNaN\t20.800000\n",
    "max\t68.120000\t7140.000000\t159.250000\tNaN\tNaN\t648.600000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the maximum fare?\n",
    "    - 159.250000\n",
    "- What is the mean distance across all trips?\n",
    "    - 8.289463 miles\n",
    "- How many cab companies are in the dataset?\n",
    "    - 31\n",
    "- What is the most frequent payment type?\n",
    "    - Credit Card\n",
    "- Are any features missing data?\n",
    "    - No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN: if the result of a calculation can not be computed or if there is missing information. For example, numeric information required for categorical features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate a correlation matrix\n",
    "- use a correlation matrix to identify features whose values correlate well with the label\n",
    "- the higher the absolute value of a correlation value is, the greater its predictive power. 0 means no correlation (not linearly related)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title Code - View correlation matrix\n",
    "training_df.corr(numeric_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    TRIP_MILES\tTRIP_SECONDS\tFARE\tTIP_RATE\n",
    "TRIP_MILES\t1.000000\t0.800855\t0.975344\t-0.049594\n",
    "TRIP_SECONDS\t0.800855\t1.000000\t0.830292\t-0.084294\n",
    "FARE\t0.975344\t0.830292\t1.000000\t-0.070979\n",
    "TIP_RATE\t-0.049594\t-0.084294\t-0.070979\t1.000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which feature correlates most strongly to the label FARE?\n",
    "    - Trip miles because it has the largest absolute value\n",
    "- Which feature correlates least strongly to the label FARE?\n",
    "    - Tip rate because of the lowest absolute value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize relationships in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title Code - View pairplot\n",
    "sns.pairplot(training_df, x_vars=[\"FARE\", \"TRIP_MILES\", \"TRIP_SECONDS\"], y_vars=[\"FARE\", \"TRIP_MILES\", \"TRIP_SECONDS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define plotting functions to view model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title Define plotting functions\n",
    "\n",
    "def make_plots(df, feature_names, label_name, model_output, sample_size=200):\n",
    "\n",
    "  random_sample = df.sample(n=sample_size).copy()\n",
    "  random_sample.reset_index()\n",
    "  weights, bias, epochs, rmse = model_output\n",
    "\n",
    "  is_2d_plot = len(feature_names) == 1\n",
    "  model_plot_type = \"scatter\" if is_2d_plot else \"surface\"\n",
    "  fig = make_subplots(rows=1, cols=2,\n",
    "                      subplot_titles=(\"Loss Curve\", \"Model Plot\"),\n",
    "                      specs=[[{\"type\": \"scatter\"}, {\"type\": model_plot_type}]])\n",
    "\n",
    "  plot_data(random_sample, feature_names, label_name, fig)\n",
    "  plot_model(random_sample, feature_names, weights, bias, fig)\n",
    "  plot_loss_curve(epochs, rmse, fig)\n",
    "\n",
    "  fig.show()\n",
    "  return\n",
    "\n",
    "def plot_loss_curve(epochs, rmse, fig):\n",
    "  curve = px.line(x=epochs, y=rmse)\n",
    "  curve.update_traces(line_color='#ff0000', line_width=3)\n",
    "\n",
    "  fig.append_trace(curve.data[0], row=1, col=1)\n",
    "  fig.update_xaxes(title_text=\"Epoch\", row=1, col=1)\n",
    "  fig.update_yaxes(title_text=\"Root Mean Squared Error\", row=1, col=1, range=[rmse.min()*0.8, rmse.max()])\n",
    "\n",
    "  return\n",
    "\n",
    "def plot_data(df, features, label, fig):\n",
    "  if len(features) == 1:\n",
    "    scatter = px.scatter(df, x=features[0], y=label)\n",
    "  else:\n",
    "    scatter = px.scatter_3d(df, x=features[0], y=features[1], z=label)\n",
    "\n",
    "  fig.append_trace(scatter.data[0], row=1, col=2)\n",
    "  if len(features) == 1:\n",
    "    fig.update_xaxes(title_text=features[0], row=1, col=2)\n",
    "    fig.update_yaxes(title_text=label, row=1, col=2)\n",
    "  else:\n",
    "    fig.update_layout(scene1=dict(xaxis_title=features[0], yaxis_title=features[1], zaxis_title=label))\n",
    "\n",
    "  return\n",
    "\n",
    "def plot_model(df, features, weights, bias, fig):\n",
    "  df['FARE_PREDICTED'] = bias[0]\n",
    "\n",
    "  for index, feature in enumerate(features):\n",
    "    df['FARE_PREDICTED'] = df['FARE_PREDICTED'] + weights[index][0] * df[feature]\n",
    "\n",
    "  if len(features) == 1:\n",
    "    model = px.line(df, x=features[0], y='FARE_PREDICTED')\n",
    "    model.update_traces(line_color='#ff0000', line_width=3)\n",
    "  else:\n",
    "    z_name, y_name = \"FARE_PREDICTED\", features[1]\n",
    "    z = [df[z_name].min(), (df[z_name].max() - df[z_name].min()) / 2, df[z_name].max()]\n",
    "    y = [df[y_name].min(), (df[y_name].max() - df[y_name].min()) / 2, df[y_name].max()]\n",
    "    x = []\n",
    "    for i in range(len(y)):\n",
    "      x.append((z[i] - weights[1][0] * y[i] - bias[0]) / weights[0][0])\n",
    "\n",
    "    plane=pd.DataFrame({'x':x, 'y':y, 'z':[z] * 3})\n",
    "\n",
    "    light_yellow = [[0, '#89CFF0'], [1, '#FFDB58']]\n",
    "    model = go.Figure(data=go.Surface(x=plane['x'], y=plane['y'], z=plane['z'],\n",
    "                                      colorscale=light_yellow))\n",
    "\n",
    "  fig.add_trace(model.data[0], row=1, col=2)\n",
    "\n",
    "  return\n",
    "\n",
    "def model_info(feature_names, label_name, model_output):\n",
    "  weights = model_output[0]\n",
    "  bias = model_output[1]\n",
    "\n",
    "  nl = \"\\n\"\n",
    "  header = \"-\" * 80\n",
    "  banner = header + nl + \"|\" + \"MODEL INFO\".center(78) + \"|\" + nl + header\n",
    "\n",
    "  info = \"\"\n",
    "  equation = label_name + \" = \"\n",
    "\n",
    "  for index, feature in enumerate(feature_names):\n",
    "    info = info + \"Weight for feature[{}]: {:.3f}\\n\".format(feature, weights[index][0])\n",
    "    equation = equation + \"{:.3f} * {} + \".format(weights[index][0], feature)\n",
    "\n",
    "  info = info + \"Bias: {:.3f}\\n\".format(bias[0])\n",
    "  equation = equation + \"{:.3f}\\n\".format(bias[0])\n",
    "\n",
    "  return banner + nl + info + nl + equation\n",
    "\n",
    "print(\"SUCCESS: defining plotting functions complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define functions to build and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title Code - Define ML functions\n",
    "\n",
    "def build_model(my_learning_rate, num_features):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "  # Describe the topography of the model.\n",
    "  # The topography of a simple linear regression model\n",
    "  # is a single node in a single layer.\n",
    "  inputs = keras.Input(shape=(num_features,))\n",
    "  outputs = keras.layers.Dense(units=1)(inputs)\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "  # Compile the model topography into code that Keras can efficiently\n",
    "  # execute. Configure training to minimize the model's mean squared error.\n",
    "  model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def train_model(model, df, features, label, epochs, batch_size):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  # Feed the model the feature and the label.\n",
    "  # The model will train for the specified number of epochs.\n",
    "  # input_x = df.iloc[:,1:3].values\n",
    "  # df[feature]\n",
    "  history = model.fit(x=features,\n",
    "                      y=label,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs)\n",
    "\n",
    "  # Gather the trained model's weight and bias.\n",
    "  trained_weight = model.get_weights()[0]\n",
    "  trained_bias = model.get_weights()[1]\n",
    "\n",
    "  # The list of epochs is stored separately from the rest of history.\n",
    "  epochs = history.epoch\n",
    "\n",
    "  # Isolate the error for each epoch.\n",
    "  hist = pd.DataFrame(history.history)\n",
    "\n",
    "  # To track the progression of training, we're going to take a snapshot\n",
    "  # of the model's root mean squared error at each epoch.\n",
    "  rmse = hist[\"root_mean_squared_error\"]\n",
    "\n",
    "  return trained_weight, trained_bias, epochs, rmse\n",
    "\n",
    "\n",
    "def run_experiment(df, feature_names, label_name, learning_rate, epochs, batch_size):\n",
    "\n",
    "  print('INFO: starting training experiment with features={} and label={}\\n'.format(feature_names, label_name))\n",
    "\n",
    "  num_features = len(feature_names)\n",
    "\n",
    "  features = df.loc[:, feature_names].values\n",
    "  label = df[label_name].values\n",
    "\n",
    "  model = build_model(learning_rate, num_features)\n",
    "  model_output = train_model(model, df, features, label, epochs, batch_size)\n",
    "\n",
    "  print('\\nSUCCESS: training experiment complete\\n')\n",
    "  print('{}'.format(model_info(feature_names, label_name, model_output)))\n",
    "  make_plots(df, feature_names, label_name, model_output)\n",
    "\n",
    "  return model\n",
    "\n",
    "print(\"SUCCESS: defining linear regression functions complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train a model with one feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title Code - Experiment 1\n",
    "\n",
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "# Specify the feature and the label.\n",
    "features = ['TRIP_MILES']\n",
    "label = 'FARE'\n",
    "\n",
    "model_1 = run_experiment(training_df, features, label, learning_rate, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "INFO: starting training experiment with features=['TRIP_MILES'] and label=FARE\n",
    "\n",
    "Epoch 1/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - loss: 348.6346 - root_mean_squared_error: 18.6426\n",
    "Epoch 2/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - loss: 133.5084 - root_mean_squared_error: 11.5241\n",
    "Epoch 3/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - loss: 34.1264 - root_mean_squared_error: 5.8180\n",
    "Epoch 4/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - loss: 16.2051 - root_mean_squared_error: 4.0221\n",
    "Epoch 5/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 3s 3ms/step - loss: 17.6679 - root_mean_squared_error: 4.1991\n",
    "Epoch 6/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - loss: 16.5441 - root_mean_squared_error: 4.0616\n",
    "Epoch 7/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 15.2725 - root_mean_squared_error: 3.9015\n",
    "Epoch 8/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 14.5931 - root_mean_squared_error: 3.8125\n",
    "Epoch 9/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 13.5662 - root_mean_squared_error: 3.6773\n",
    "Epoch 10/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 14.7848 - root_mean_squared_error: 3.8370\n",
    "Epoch 11/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 14.3020 - root_mean_squared_error: 3.7787\n",
    "Epoch 12/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 13.7281 - root_mean_squared_error: 3.6994\n",
    "Epoch 13/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 14.0061 - root_mean_squared_error: 3.7396\n",
    "Epoch 14/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - loss: 15.4342 - root_mean_squared_error: 3.9199\n",
    "Epoch 15/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - loss: 14.1800 - root_mean_squared_error: 3.7613\n",
    "Epoch 16/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 15.0049 - root_mean_squared_error: 3.8598\n",
    "Epoch 17/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 13.5835 - root_mean_squared_error: 3.6804\n",
    "Epoch 18/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 14.3659 - root_mean_squared_error: 3.7696\n",
    "Epoch 19/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 14.5944 - root_mean_squared_error: 3.8178\n",
    "Epoch 20/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 14.2164 - root_mean_squared_error: 3.7658\n",
    "\n",
    "SUCCESS: training experiment complete\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "|                                  MODEL INFO                                  |\n",
    "--------------------------------------------------------------------------------\n",
    "Weight for feature[TRIP_MILES]: 2.277\n",
    "Bias: 4.970\n",
    "\n",
    "FARE = 2.277 * TRIP_MILES + 4.970\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many epochs did it take to converge on the final model?\n",
    "    - 9 because after 9 epochs, error increases and it has the minimum error value\n",
    "- How well does the model fit the sample data?\n",
    "    - pretty well\n",
    "\n",
    "- Ans：\n",
    "    - Use the **loss curve** to see where the loss begins to level off during training.\n",
    "    - With this set of hyperparameters: learning_rate = 0.001，epochs = 20，batch_size = 50\n",
    "    - it takes about 5 epochs for the training run to converge to the final model.\n",
    "    - It appears from the model plot that the model fits the sample data fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment with hyperparameters\n",
    "- Experiment 1: Increase the learning rate to 1 (batch size at 50).\n",
    "- Experiment 2: Decrease the learning rate to 0.0001 (batch size at 50).\n",
    "- Experiment 3: Increase the batch size to 500 (learning rate at 0.001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Experiment 1\n",
    "INFO: starting training experiment with features=['TRIP_MILES'] and label=FARE\n",
    "\n",
    "Epoch 1/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 64.4992 - root_mean_squared_error: 7.6212\n",
    "Epoch 2/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 45.1373 - root_mean_squared_error: 6.6933\n",
    "Epoch 3/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 47.7835 - root_mean_squared_error: 6.9096\n",
    "Epoch 4/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 45.2509 - root_mean_squared_error: 6.7005\n",
    "Epoch 5/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 47.7799 - root_mean_squared_error: 6.8892\n",
    "Epoch 6/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 46.4212 - root_mean_squared_error: 6.7982\n",
    "Epoch 7/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 45.5827 - root_mean_squared_error: 6.7395\n",
    "Epoch 8/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - loss: 48.0836 - root_mean_squared_error: 6.9301\n",
    "Epoch 9/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - loss: 47.3604 - root_mean_squared_error: 6.8767\n",
    "Epoch 10/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 48.5620 - root_mean_squared_error: 6.9658\n",
    "Epoch 11/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 46.2598 - root_mean_squared_error: 6.7891\n",
    "Epoch 12/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 45.9575 - root_mean_squared_error: 6.7713\n",
    "Epoch 13/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 48.8434 - root_mean_squared_error: 6.9865\n",
    "Epoch 14/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 45.7235 - root_mean_squared_error: 6.7424\n",
    "Epoch 15/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 46.7988 - root_mean_squared_error: 6.8387\n",
    "Epoch 16/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 47.8109 - root_mean_squared_error: 6.9128\n",
    "Epoch 17/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - loss: 51.0231 - root_mean_squared_error: 7.1289\n",
    "Epoch 18/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - loss: 48.4777 - root_mean_squared_error: 6.9601\n",
    "Epoch 19/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 48.9496 - root_mean_squared_error: 6.9924\n",
    "Epoch 20/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 48.2791 - root_mean_squared_error: 6.9457\n",
    "\n",
    "SUCCESS: training experiment complete\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "|                                  MODEL INFO                                  |\n",
    "--------------------------------------------------------------------------------\n",
    "Weight for feature[TRIP_MILES]: 2.646\n",
    "Bias: 4.974\n",
    "\n",
    "FARE = 2.646 * TRIP_MILES + 4.974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Experiment 3\n",
    "INFO: starting training experiment with features=['TRIP_MILES'] and label=FARE\n",
    "\n",
    "Epoch 1/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 346.5096 - root_mean_squared_error: 18.6143\n",
    "Epoch 2/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 316.7681 - root_mean_squared_error: 17.7977\n",
    "Epoch 3/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 296.7084 - root_mean_squared_error: 17.2241\n",
    "Epoch 4/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 270.0554 - root_mean_squared_error: 16.4328\n",
    "Epoch 5/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 245.9910 - root_mean_squared_error: 15.6837\n",
    "Epoch 6/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 220.2236 - root_mean_squared_error: 14.8398\n",
    "Epoch 7/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 202.0853 - root_mean_squared_error: 14.2153\n",
    "Epoch 8/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 182.0072 - root_mean_squared_error: 13.4908\n",
    "Epoch 9/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 163.9141 - root_mean_squared_error: 12.8027\n",
    "Epoch 10/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 149.6938 - root_mean_squared_error: 12.2338\n",
    "Epoch 11/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 132.0897 - root_mean_squared_error: 11.4913\n",
    "Epoch 12/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 115.6083 - root_mean_squared_error: 10.7517\n",
    "Epoch 13/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 103.9859 - root_mean_squared_error: 10.1946\n",
    "Epoch 14/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 89.7640 - root_mean_squared_error: 9.4732\n",
    "Epoch 15/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 76.9155 - root_mean_squared_error: 8.7699\n",
    "Epoch 16/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 65.8133 - root_mean_squared_error: 8.1118\n",
    "Epoch 17/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 54.3047 - root_mean_squared_error: 7.3690\n",
    "Epoch 18/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 47.4587 - root_mean_squared_error: 6.8881\n",
    "Epoch 19/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 43.1465 - root_mean_squared_error: 6.5618\n",
    "Epoch 20/20\n",
    "64/64 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 34.1069 - root_mean_squared_error: 5.8392\n",
    "\n",
    "SUCCESS: training experiment complete\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "|                                  MODEL INFO                                  |\n",
    "--------------------------------------------------------------------------------\n",
    "Weight for feature[TRIP_MILES]: 2.241\n",
    "Bias: 1.276\n",
    "\n",
    "FARE = 2.241 * TRIP_MILES + 1.276"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How did raising the learning rate impact your ability to train the model?\n",
    "    - based on the loss curve, the model doesn't converge (fluctuate)\n",
    "- How did lowering the learning rate impact your ability to train the model?\n",
    "    - based on the loss curve, the model converge slowly，but not yet converge even after epochs finished, expect larger epochs for it to be converged.\n",
    "- Did changing the batch size effect your training results?\n",
    "    - until epochs finished, the curve doesn't converge. Based on previous lectures, large batch size can reduce impact by outliers while small batch size can help to train better model while introducing noises. \n",
    "\n",
    "- Ans:\n",
    "    - When the learning rate is too high, the loss curve bounces around and does not\n",
    "appear to be moving towards convergence with each iteration. Also, notice that\n",
    "the predicted model does not fit the data very well. With a learning rate that\n",
    "is too high, it is unlikely that you will be able to train a model with good\n",
    "results.\n",
    "    - When the learning rate is too small, it may take longer for the loss curve to\n",
    "converge. With a small learning rate the loss curve decreases slowly, but does\n",
    "not show a dramatic drop or leveling off. With a small learning rate you could\n",
    "increase the number of epochs so that your model will eventually converge, but\n",
    "it will take longer.\n",
    "    - Increasing the batch size makes each epoch run faster, but as with the smaller\n",
    "learning rate, the model does not converge with just 20 epochs. If you have\n",
    "time, try increasing the number of epochs and eventually you should see the\n",
    "model converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra experiment done by myself: make batch size = 5\n",
    "- each epoch finish much slower than when having larger batch size \n",
    "- but can see convergence while large batch size couldn't converge within the same number of epochs\n",
    "- see a really deep drop for loss curve to converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train a model with two features\n",
    "- In this step, try training the model with two features, TRIP_MILES and TRIP_MINUTES, to see if you can improve the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title Code - Experiment 3\n",
    "\n",
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "training_df.loc[:, 'TRIP_MINUTES'] = training_df['TRIP_SECONDS']/60\n",
    "\n",
    "features = ['TRIP_MILES', 'TRIP_MINUTES']  # features = ['TRIP_MILES', 'TRIP_SECONDS']\n",
    "label = 'FARE'\n",
    "\n",
    "model_2 = run_experiment(training_df, features, label, learning_rate, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "INFO: starting training experiment with features=['TRIP_MILES', 'TRIP_MINUTES'] and label=FARE\n",
    "\n",
    "Epoch 1/20\n",
    "<ipython-input-19-8892146dfcd1>:8: SettingWithCopyWarning:\n",
    "\n",
    "\n",
    "A value is trying to be set on a copy of a slice from a DataFrame.\n",
    "Try using .loc[row_indexer,col_indexer] = value instead\n",
    "\n",
    "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
    "\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - loss: 96.0879 - root_mean_squared_error: 9.7069\n",
    "Epoch 2/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - loss: 31.2299 - root_mean_squared_error: 5.5792\n",
    "Epoch 3/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - loss: 20.7375 - root_mean_squared_error: 4.5379\n",
    "Epoch 4/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 14.4589 - root_mean_squared_error: 3.7997\n",
    "Epoch 5/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 13.8762 - root_mean_squared_error: 3.7199\n",
    "Epoch 6/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 13.5240 - root_mean_squared_error: 3.6698\n",
    "Epoch 7/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 12.1855 - root_mean_squared_error: 3.4879\n",
    "Epoch 8/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 12.6429 - root_mean_squared_error: 3.5509\n",
    "Epoch 9/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 14.1179 - root_mean_squared_error: 3.7507\n",
    "Epoch 10/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 11.4985 - root_mean_squared_error: 3.3820\n",
    "Epoch 11/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 11.2461 - root_mean_squared_error: 3.3477\n",
    "Epoch 12/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - loss: 12.0351 - root_mean_squared_error: 3.4657\n",
    "Epoch 13/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 11.7800 - root_mean_squared_error: 3.4179\n",
    "Epoch 14/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 14.1723 - root_mean_squared_error: 3.7434\n",
    "Epoch 15/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 13.0145 - root_mean_squared_error: 3.6026\n",
    "Epoch 16/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 11.4771 - root_mean_squared_error: 3.3826\n",
    "Epoch 17/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 11.2512 - root_mean_squared_error: 3.3514\n",
    "Epoch 18/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step - loss: 12.0794 - root_mean_squared_error: 3.4682\n",
    "Epoch 19/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - loss: 12.1967 - root_mean_squared_error: 3.4893\n",
    "Epoch 20/20\n",
    "634/634 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - loss: 12.1323 - root_mean_squared_error: 3.4750\n",
    "\n",
    "SUCCESS: training experiment complete\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "|                                  MODEL INFO                                  |\n",
    "--------------------------------------------------------------------------------\n",
    "Weight for feature[TRIP_MILES]: 2.027\n",
    "Weight for feature[TRIP_MINUTES]: 0.147\n",
    "Bias: 3.833\n",
    "\n",
    "FARE = 2.027 * TRIP_MILES + 0.147 * TRIP_MINUTES + 3.833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------------\n",
    "|                                  MODEL INFO                                  |\n",
    "--------------------------------------------------------------------------------\n",
    "Weight for feature[TRIP_MILES]: 2.021\n",
    "Weight for feature[TRIP_SECONDS]: 0.002\n",
    "Bias: 3.645\n",
    "\n",
    "FARE = 2.021 * TRIP_MILES + 0.002 * TRIP_SECONDS + 3.645"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Does the model with two features produce better results than one using a single feature?\n",
    "    - yes, with the same setting of parameters (0.001, 20, 50) two features has lower root mean squared error than one feature\n",
    "- Does it make a difference if you use TRIP_SECONDS instead of TRIP_MINUTES?\n",
    "    - based on the experiment, using trip_seconds, the loss curve converges really quickly at epoch = 2, while for trip_minutes, it was epoch=5\n",
    "- How well do you think the model comes to the ground truth fare calculation for Chicago Taxi Trips?\n",
    "    - based on the model plot, it predicts pretty well.\n",
    "\n",
    "- Ans:\n",
    "    - To answer this question for your specific training runs, compare the RMSE for\n",
    "each model. For example, if the RMSE for the model trained with one feature was\n",
    "3.7457 and the RMSE for the model with two features is 3.4787, that means that\n",
    "on average the model with two features makes predictions that are about $0.27\n",
    "closer to the observed fare.\n",
    "\n",
    "    - When training a model with more than one feature, it is important that all\n",
    "numeric values are roughly on the same scale. In this case, TRIP_SECONDS and\n",
    "TRIP_MILES do not meet this criteria. The mean value for TRIP_MILES is 8.3 and\n",
    "the mean for TRIP_SECONDS is 1320; that is two orders of magnitude difference.\n",
    "Converting the trip duration to minutes helps during training because in puts\n",
    "values for both features on a more comparable scale. Of course, this is not the\n",
    "only way to scale values before training, but you will learn about that in\n",
    "another module.\n",
    "\n",
    "    - In reality, Chicago taxi cabs use a documented formula to determine cab fares.\n",
    "For a single passenger paying cash, the fare is calculated like this: FARE = 2.25 * TRIP_MILES + 0.12 * TRIP_MINUTES + 3.25\n",
    "\n",
    "    - Typically with machine learning problems you would not know the 'correct'\n",
    "formula, but in this case you can this knowledge to evaluate your model. Take a\n",
    "look at your model output (the weights and bias) and determine how well it\n",
    "matches the ground truth fare calculation. You should find that the model is\n",
    "roughly close to this formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4 - Validate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title Code - Define functions to make predictions\n",
    "def format_currency(x):\n",
    "  return \"${:.2f}\".format(x)\n",
    "\n",
    "def build_batch(df, batch_size):\n",
    "  batch = df.sample(n=batch_size).copy()\n",
    "  batch.set_index(np.arange(batch_size), inplace=True)\n",
    "  return batch\n",
    "\n",
    "def predict_fare(model, df, features, label, batch_size=50):\n",
    "  batch = build_batch(df, batch_size)\n",
    "  predicted_values = model.predict_on_batch(x=batch.loc[:, features].values)\n",
    "\n",
    "  data = {\"PREDICTED_FARE\": [], \"OBSERVED_FARE\": [], \"L1_LOSS\": [],\n",
    "          features[0]: [], features[1]: []}\n",
    "  for i in range(batch_size):\n",
    "    predicted = predicted_values[i][0]\n",
    "    observed = batch.at[i, label]\n",
    "    data[\"PREDICTED_FARE\"].append(format_currency(predicted))\n",
    "    data[\"OBSERVED_FARE\"].append(format_currency(observed))\n",
    "    data[\"L1_LOSS\"].append(format_currency(abs(observed - predicted)))\n",
    "    data[features[0]].append(batch.at[i, features[0]])\n",
    "    data[features[1]].append(\"{:.2f}\".format(batch.at[i, features[1]]))\n",
    "\n",
    "  output_df = pd.DataFrame(data)\n",
    "  return output_df\n",
    "\n",
    "def show_predictions(output):\n",
    "  header = \"-\" * 80\n",
    "  banner = header + \"\\n\" + \"|\" + \"PREDICTIONS\".center(78) + \"|\" + \"\\n\" + header\n",
    "  print(banner)\n",
    "  print(output)\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title Code - Make predictions\n",
    "\n",
    "output = predict_fare(model_2, training_df, features, label)\n",
    "show_predictions(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------------\n",
    "|                                 PREDICTIONS                                  |\n",
    "--------------------------------------------------------------------------------\n",
    "   PREDICTED_FARE OBSERVED_FARE L1_LOSS  TRIP_MILES TRIP_SECONDS\n",
    "0           $5.71         $5.50   $0.21        0.66       294.00\n",
    "1          $11.55        $13.25   $1.70        2.00      1560.00\n",
    "2          $23.48        $23.75   $0.27        7.90      1562.00\n",
    "3           $8.60         $8.75   $0.15        1.69       622.00\n",
    "4           $5.80         $5.25   $0.55        0.80       217.00\n",
    "5          $10.31        $10.00   $0.31        2.30       817.00\n",
    "6          $19.26        $19.74   $0.48        6.63       897.00\n",
    "7          $28.19        $28.00   $0.19       10.60      1260.00\n",
    "8          $14.33        $13.75   $0.58        4.61       552.00\n",
    "9          $13.22        $13.50   $0.28        3.59       936.00\n",
    "10          $9.18         $9.50   $0.32        1.66       883.00\n",
    "11          $9.45         $9.50   $0.05        2.06       663.00\n",
    "12         $31.17        $31.50   $0.33       12.00      1320.00\n",
    "13         $52.23        $53.00   $0.77       21.73      1886.00\n",
    "14         $15.73        $15.50   $0.23        5.10       720.00\n",
    "15          $7.12         $6.75   $0.37        1.21       418.00\n",
    "16          $6.15         $6.00   $0.15        0.80       360.00\n",
    "17         $42.66        $42.75   $0.09       17.10      1800.00\n",
    "18         $25.61        $25.75   $0.14        9.50      1118.00\n",
    "19         $30.19        $30.50   $0.31       12.09       850.00\n",
    "20          $9.33         $9.25   $0.08        1.96       697.00\n",
    "21         $26.95        $26.75   $0.20       10.04      1219.00\n",
    "22         $30.29        $29.50   $0.79       10.10      2520.00\n",
    "23         $52.21        $55.25   $3.04       19.48      3716.00\n",
    "24          $5.96         $5.50   $0.46        0.85       240.00\n",
    "25         $41.87        $42.00   $0.13       17.00      1560.00\n",
    "26          $7.66         $7.50   $0.16        1.30       562.00\n",
    "27         $40.88        $41.50   $0.62       16.59      1497.00\n",
    "28          $8.92         $8.75   $0.17        1.80       660.00\n",
    "29          $9.77         $9.50   $0.27        2.20       678.00\n",
    "30         $61.13        $59.75   $1.38       24.79      2982.00\n",
    "31         $19.21        $19.74   $0.53        6.55       942.00\n",
    "32         $29.65        $29.75   $0.10       10.52      1917.00\n",
    "33         $24.71        $24.50   $0.21        8.00      1980.00\n",
    "34         $10.08         $9.50   $0.58        2.10       887.00\n",
    "35          $5.15         $4.75   $0.40        0.60       117.00\n",
    "36         $26.24        $29.00   $2.76        8.54      2156.00\n",
    "37          $5.96         $5.50   $0.46        0.82       267.00\n",
    "38         $15.47        $17.75   $2.28        3.50      1920.00\n",
    "39          $8.25         $8.00   $0.25        1.62       538.00\n",
    "40         $10.78        $10.25   $0.53        2.64       726.00\n",
    "41          $6.61         $6.25   $0.36        0.99       391.00\n",
    "42         $10.49        $10.46   $0.03        2.31       879.00\n",
    "43         $12.29        $14.75   $2.46        2.34      1584.00\n",
    "44         $45.13        $45.25   $0.12       18.17      1925.00\n",
    "45          $8.07         $8.00   $0.07        1.60       480.00\n",
    "46         $31.72        $31.25   $0.47       11.10      2280.00\n",
    "47         $12.17        $12.50   $0.33        2.87      1100.00\n",
    "48         $38.50        $38.50   $0.00       14.74      2045.00\n",
    "49         $29.50        $29.50   $0.00       11.27      1245.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How close is the predicted value to the label value? In other words, does your model accurately predict the fare for a taxi ride?\n",
    "    - they are pretty close, most of them are < $0.5\n",
    "\n",
    "- Ans:\n",
    "    - Based on a random sampling of examples, the model seems to do pretty well\n",
    "predicting the fare for a taxi ride. Most of the predicted values do not vary\n",
    "significantly from the observed value. You should be able to see this by looking\n",
    "at the column L1_LOSS = |observed - predicted|."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
