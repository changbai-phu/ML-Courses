{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "- convert raw model output to a value between 0 and 1 to predict the probabilities.\n",
    "- typically two ways:\n",
    "    - 'As is': e.g., 0.92 means 92% the email is spam\n",
    "    - binary category: convert the output to binary category using classification. \n",
    "- logistic functions: used to ensure the output represents probability\n",
    "    - Sigmoid function（s-shaped）: $y=\\frac{1}{1 + \\exp(-z)}$ \n",
    "        - transform linear output using the sigmoid function \n",
    "        - y has values between 0 and 1 \n",
    "        - z = b + $w_0x_0$ + $w_1x_1$ + ... + $w_nx_n$\n",
    "        - z is the output of the liear equation, called log odds because z = $log(\\frac{y}{1-y})$\n",
    "        - b is bias, w are learned weights, x are feature values\n",
    "        - property of logarithm: $-ln(x) = ln(\\frac{1}{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss and regularization\n",
    "- unlike linear regression using squared loss ($L_2$ loss), for logistic regression, due to its s-shape property, we use **log loss** to calculate the loss. \n",
    "    - because as number of features increase, it will need more memory to preserve the precise to calculate the loss. \n",
    "    - **log loss** = $\\sum{-ylog(y')+(1-y)log(1-y')}$, where y' is predicted value between 0 and 1, y is actual value either 0 or 1\n",
    "- Regularization: used to reduce complexity of logistic model\n",
    "    - $L_2$ regularization\n",
    "    - early stopping "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
