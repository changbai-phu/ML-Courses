{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is copied from: https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "print(f\"Found TF-DF {tfdf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "serving_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "PassengerId\tSurvived\tPclass\tName\tSex\tAge\tSibSp\tParch\tTicket\tFare\tCabin\tEmbarked\n",
    "0\t1\t0\t3\tBraund, Mr. Owen Harris\tmale\t22.0\t1\t0\tA/5 21171\t7.2500\tNaN\tS\n",
    "1\t2\t1\t1\tCumings, Mrs. John Bradley (Florence Briggs Th...\tfemale\t38.0\t1\t0\tPC 17599\t71.2833\tC85\tC\n",
    "2\t3\t1\t3\tHeikkinen, Miss. Laina\tfemale\t26.0\t0\t0\tSTON/O2. 3101282\t7.9250\tNaN\tS\n",
    "3\t4\t1\t1\tFutrelle, Mrs. Jacques Heath (Lily May Peel)\tfemale\t35.0\t1\t0\t113803\t53.1000\tC123\tS\n",
    "4\t5\t0\t3\tAllen, Mr. William Henry\tmale\t35.0\t0\t0\t373450\t8.0500\tNaN\tS\n",
    "5\t6\t0\t3\tMoran, Mr. James\tmale\tNaN\t0\t0\t330877\t8.4583\tNaN\tQ\n",
    "6\t7\t0\t1\tMcCarthy, Mr. Timothy J\tmale\t54.0\t0\t0\t17463\t51.8625\tE46\tS\n",
    "7\t8\t0\t3\tPalsson, Master. Gosta Leonard\tmale\t2.0\t3\t1\t349909\t21.0750\tNaN\tS\n",
    "8\t9\t1\t3\tJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\tfemale\t27.0\t0\t2\t347742\t11.1333\tNaN\tS\n",
    "9\t10\t1\t2\tNasser, Mrs. Nicholas (Adele Achem)\tfemale\t14.0\t1\t0\t237736\t30.0708\tNaN\tC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "Apply folowing transformations: \n",
    "1. Tokenize the names. For example, \"Braund, Mr. Owen Harris\" will become [\"Braund\", \"Mr.\", \"Owen\", \"Harris\"].\n",
    "2. Extract any prefix in the ticket. For example ticket \"STON/O2. 3101282\" will become \"STON/O2.\" and 3101282."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    def normalize_name(x):\n",
    "        return \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n",
    "    \n",
    "    def ticket_number(x):\n",
    "        return x.split(\" \")[-1]\n",
    "        \n",
    "    def ticket_item(x):\n",
    "        items = x.split(\" \")\n",
    "        if len(items) == 1:\n",
    "            return \"NONE\"\n",
    "        return \"_\".join(items[0:-1])\n",
    "    \n",
    "    df[\"Name\"] = df[\"Name\"].apply(normalize_name)\n",
    "    df[\"Ticket_number\"] = df[\"Ticket\"].apply(ticket_number)\n",
    "    df[\"Ticket_item\"] = df[\"Ticket\"].apply(ticket_item)                     \n",
    "    return df\n",
    "    \n",
    "preprocessed_train_df = preprocess(train_df)\n",
    "preprocessed_serving_df = preprocess(serving_df)\n",
    "\n",
    "preprocessed_train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "PassengerId\tSurvived\tPclass\tName\tSex\tAge\tSibSp\tParch\tTicket\tFare\tCabin\tEmbarked\tTicket_number\tTicket_item\n",
    "0\t1\t0\t3\tBraund Mr Owen Harris\tmale\t22.0\t1\t0\tA/5 21171\t7.2500\tNaN\tS\t21171\tA/5\n",
    "1\t2\t1\t1\tCumings Mrs John Bradley Florence Briggs Thayer\tfemale\t38.0\t1\t0\tPC 17599\t71.2833\tC85\tC\t17599\tPC\n",
    "2\t3\t1\t3\tHeikkinen Miss Laina\tfemale\t26.0\t0\t0\tSTON/O2. 3101282\t7.9250\tNaN\tS\t3101282\tSTON/O2.\n",
    "3\t4\t1\t1\tFutrelle Mrs Jacques Heath Lily May Peel\tfemale\t35.0\t1\t0\t113803\t53.1000\tC123\tS\t113803\tNONE\n",
    "4\t5\t0\t3\tAllen Mr William Henry\tmale\t35.0\t0\t0\t373450\t8.0500\tNaN\tS\t373450\tNONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "input_features = list(preprocessed_train_df.columns)\n",
    "input_features.remove(\"Ticket\")\n",
    "input_features.remove(\"PassengerId\")\n",
    "input_features.remove(\"Survived\")\n",
    "#input_features.remove(\"Ticket_number\")\n",
    "\n",
    "print(f\"Input features: {input_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Input features: ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked', 'Ticket_number', 'Ticket_item']\n",
    "add Codeadd Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Pandas dataset to TensorFlow DatasetÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_names(features, labels=None):\n",
    "    \"\"\"Divite the names into tokens. TF-DF can consume text tokens natively.\"\"\"\n",
    "    features[\"Name\"] =  tf.strings.split(features[\"Name\"])\n",
    "    return features, labels\n",
    "\n",
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_train_df,label=\"Survived\").map(tokenize_names)\n",
    "serving_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_serving_df).map(tokenize_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with default parameters\n",
    "Train model\n",
    "- First, we are training a GradientBoostedTreesModel model with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = tfdf.keras.GradientBoostedTreesModel(\n",
    "    verbose=0, # Very few logs\n",
    "    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n",
    "    exclude_non_specified_features=True, # Only use the features in \"features\"\n",
    "    random_seed=1234,\n",
    ")\n",
    "model.fit(train_ds)\n",
    "\n",
    "self_evaluation = model.make_inspector().evaluation()\n",
    "print(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.8260869383811951 Loss:0.8608942627906799"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with improved default parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = tfdf.keras.GradientBoostedTreesModel(\n",
    "    verbose=0, # Very few logs\n",
    "    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n",
    "    exclude_non_specified_features=True, # Only use the features in \"features\"\n",
    "    \n",
    "    #num_trees=2000,\n",
    "    \n",
    "    # Only for GBT.\n",
    "    # A bit slower, but great to understand the model.\n",
    "    # compute_permutation_variable_importance=True,\n",
    "    \n",
    "    # Change the default hyper-parameters\n",
    "    # hyperparameter_template=\"benchmark_rank1@v1\",\n",
    "    \n",
    "    #num_trees=1000,\n",
    "    #tuner=tuner\n",
    "    \n",
    "    min_examples=1,\n",
    "    categorical_algorithm=\"RANDOM\",\n",
    "    #max_depth=4,\n",
    "    shrinkage=0.05,\n",
    "    #num_candidate_attributes_ratio=0.2,\n",
    "    split_axis=\"SPARSE_OBLIQUE\",\n",
    "    sparse_oblique_normalization=\"MIN_MAX\",\n",
    "    sparse_oblique_num_projections_exponent=2.0,\n",
    "    num_trees=2000,\n",
    "    #validation_ratio=0.0,\n",
    "    random_seed=1234,\n",
    "    \n",
    ")\n",
    "model.fit(train_ds)\n",
    "\n",
    "self_evaluation = model.make_inspector().evaluation()\n",
    "print(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Accuracy: 0.760869562625885 Loss:1.0154211521148682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Model: \"gradient_boosted_trees_model_1\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    "=================================================================\n",
    "Total params: 1\n",
    "Trainable params: 0\n",
    "Non-trainable params: 1\n",
    "_________________________________________________________________\n",
    "Type: \"GRADIENT_BOOSTED_TREES\"\n",
    "Task: CLASSIFICATION\n",
    "Label: \"__LABEL\"\n",
    "\n",
    "Input Features (11):\n",
    "\tAge\n",
    "\tCabin\n",
    "\tEmbarked\n",
    "\tFare\n",
    "\tName\n",
    "\tParch\n",
    "\tPclass\n",
    "\tSex\n",
    "\tSibSp\n",
    "\tTicket_item\n",
    "\tTicket_number\n",
    "\n",
    "No weights\n",
    "\n",
    "Variable Importance: INV_MEAN_MIN_DEPTH:\n",
    "    1.           \"Sex\"  0.576632 ################\n",
    "    2.           \"Age\"  0.364297 #######\n",
    "    3.          \"Fare\"  0.278839 ####\n",
    "    4.          \"Name\"  0.208548 #\n",
    "    5. \"Ticket_number\"  0.180792 \n",
    "    6.        \"Pclass\"  0.176962 \n",
    "    7.         \"Parch\"  0.176659 \n",
    "    8.   \"Ticket_item\"  0.175540 \n",
    "    9.      \"Embarked\"  0.172339 \n",
    "   10.         \"SibSp\"  0.170442 \n",
    "\n",
    "Variable Importance: NUM_AS_ROOT:\n",
    "    1.  \"Sex\" 28.000000 ################\n",
    "    2. \"Name\"  5.000000 \n",
    "\n",
    "Variable Importance: NUM_NODES:\n",
    "    1.           \"Age\" 406.000000 ################\n",
    "    2.          \"Fare\" 290.000000 ###########\n",
    "    3.          \"Name\" 44.000000 #\n",
    "    4.   \"Ticket_item\" 42.000000 #\n",
    "    5.           \"Sex\" 31.000000 #\n",
    "    6.         \"Parch\" 28.000000 \n",
    "    7. \"Ticket_number\" 22.000000 \n",
    "    8.        \"Pclass\" 15.000000 \n",
    "    9.      \"Embarked\" 12.000000 \n",
    "   10.         \"SibSp\"  5.000000 \n",
    "\n",
    "Variable Importance: SUM_SCORE:\n",
    "    1.           \"Sex\" 460.497828 ################\n",
    "    2.           \"Age\" 355.963333 ############\n",
    "    3.          \"Fare\" 292.870316 ##########\n",
    "    4.          \"Name\" 108.548952 ###\n",
    "    5.        \"Pclass\" 28.132254 \n",
    "    6.   \"Ticket_item\" 23.818676 \n",
    "    7. \"Ticket_number\" 23.772288 \n",
    "    8.         \"Parch\" 19.303155 \n",
    "    9.      \"Embarked\"  8.155722 \n",
    "   10.         \"SibSp\"  0.015225 \n",
    "\n",
    "\n",
    "\n",
    "Loss: BINOMIAL_LOG_LIKELIHOOD\n",
    "Validation loss value: 1.01542\n",
    "Number of trees per iteration: 1\n",
    "Node format: NOT_SET\n",
    "Number of trees: 33\n",
    "Total number of nodes: 1823\n",
    "\n",
    "Number of nodes by tree:\n",
    "Count: 33 Average: 55.2424 StdDev: 5.13473\n",
    "Min: 39 Max: 63 Ignored: 0\n",
    "----------------------------------------------\n",
    "[ 39, 40) 1   3.03%   3.03% #\n",
    "[ 40, 41) 0   0.00%   3.03%\n",
    "[ 41, 42) 0   0.00%   3.03%\n",
    "[ 42, 44) 0   0.00%   3.03%\n",
    "[ 44, 45) 0   0.00%   3.03%\n",
    "[ 45, 46) 0   0.00%   3.03%\n",
    "[ 46, 47) 0   0.00%   3.03%\n",
    "[ 47, 49) 2   6.06%   9.09% ###\n",
    "[ 49, 50) 2   6.06%  15.15% ###\n",
    "[ 50, 51) 0   0.00%  15.15%\n",
    "[ 51, 52) 2   6.06%  21.21% ###\n",
    "[ 52, 54) 5  15.15%  36.36% #######\n",
    "[ 54, 55) 0   0.00%  36.36%\n",
    "[ 55, 56) 5  15.15%  51.52% #######\n",
    "[ 56, 57) 0   0.00%  51.52%\n",
    "[ 57, 59) 4  12.12%  63.64% ######\n",
    "[ 59, 60) 7  21.21%  84.85% ##########\n",
    "[ 60, 61) 0   0.00%  84.85%\n",
    "[ 61, 62) 3   9.09%  93.94% ####\n",
    "[ 62, 63] 2   6.06% 100.00% ###\n",
    "\n",
    "Depth by leafs:\n",
    "Count: 928 Average: 4.8847 StdDev: 0.380934\n",
    "Min: 2 Max: 5 Ignored: 0\n",
    "----------------------------------------------\n",
    "[ 2, 3)   1   0.11%   0.11%\n",
    "[ 3, 4)  17   1.83%   1.94%\n",
    "[ 4, 5)  70   7.54%   9.48% #\n",
    "[ 5, 5] 840  90.52% 100.00% ##########\n",
    "\n",
    "Number of training obs by leaf:\n",
    "Count: 928 Average: 28.4127 StdDev: 70.8313\n",
    "Min: 1 Max: 438 Ignored: 0\n",
    "----------------------------------------------\n",
    "[   1,  22) 731  78.77%  78.77% ##########\n",
    "[  22,  44)  74   7.97%  86.75% #\n",
    "[  44,  66)  37   3.99%  90.73% #\n",
    "[  66,  88)   3   0.32%  91.06%\n",
    "[  88, 110)   9   0.97%  92.03%\n",
    "[ 110, 132)   8   0.86%  92.89%\n",
    "[ 132, 154)  18   1.94%  94.83%\n",
    "[ 154, 176)   8   0.86%  95.69%\n",
    "[ 176, 198)   6   0.65%  96.34%\n",
    "[ 198, 220)   2   0.22%  96.55%\n",
    "[ 220, 241)   2   0.22%  96.77%\n",
    "[ 241, 263)   1   0.11%  96.88%\n",
    "[ 263, 285)   2   0.22%  97.09%\n",
    "[ 285, 307)   5   0.54%  97.63%\n",
    "[ 307, 329)   1   0.11%  97.74%\n",
    "[ 329, 351)   2   0.22%  97.95%\n",
    "[ 351, 373)   6   0.65%  98.60%\n",
    "[ 373, 395)   6   0.65%  99.25%\n",
    "[ 395, 417)   2   0.22%  99.46%\n",
    "[ 417, 438]   5   0.54% 100.00%\n",
    "\n",
    "Attribute in nodes:\n",
    "\t406 : Age [NUMERICAL]\n",
    "\t290 : Fare [NUMERICAL]\n",
    "\t44 : Name [CATEGORICAL_SET]\n",
    "\t42 : Ticket_item [CATEGORICAL]\n",
    "\t31 : Sex [CATEGORICAL]\n",
    "\t28 : Parch [NUMERICAL]\n",
    "\t22 : Ticket_number [CATEGORICAL]\n",
    "\t15 : Pclass [NUMERICAL]\n",
    "\t12 : Embarked [CATEGORICAL]\n",
    "\t5 : SibSp [NUMERICAL]\n",
    "\n",
    "Attribute in nodes with depth <= 0:\n",
    "\t28 : Sex [CATEGORICAL]\n",
    "\t5 : Name [CATEGORICAL_SET]\n",
    "\n",
    "Attribute in nodes with depth <= 1:\n",
    "\t39 : Age [NUMERICAL]\n",
    "\t28 : Sex [CATEGORICAL]\n",
    "\t21 : Fare [NUMERICAL]\n",
    "\t5 : Name [CATEGORICAL_SET]\n",
    "\t3 : Pclass [NUMERICAL]\n",
    "\t2 : Ticket_number [CATEGORICAL]\n",
    "\t1 : Parch [NUMERICAL]\n",
    "\n",
    "Attribute in nodes with depth <= 2:\n",
    "\t102 : Age [NUMERICAL]\n",
    "\t65 : Fare [NUMERICAL]\n",
    "\t28 : Sex [CATEGORICAL]\n",
    "\t15 : Name [CATEGORICAL_SET]\n",
    "\t7 : Ticket_number [CATEGORICAL]\n",
    "\t5 : Pclass [NUMERICAL]\n",
    "\t4 : Parch [NUMERICAL]\n",
    "\t2 : Ticket_item [CATEGORICAL]\n",
    "\t2 : Embarked [CATEGORICAL]\n",
    "\n",
    "Attribute in nodes with depth <= 3:\n",
    "\t206 : Age [NUMERICAL]\n",
    "\t156 : Fare [NUMERICAL]\n",
    "\t33 : Name [CATEGORICAL_SET]\n",
    "\t29 : Sex [CATEGORICAL]\n",
    "\t19 : Ticket_number [CATEGORICAL]\n",
    "\t11 : Ticket_item [CATEGORICAL]\n",
    "\t11 : Parch [NUMERICAL]\n",
    "\t7 : Pclass [NUMERICAL]\n",
    "\t3 : Embarked [CATEGORICAL]\n",
    "\n",
    "Attribute in nodes with depth <= 5:\n",
    "\t406 : Age [NUMERICAL]\n",
    "\t290 : Fare [NUMERICAL]\n",
    "\t44 : Name [CATEGORICAL_SET]\n",
    "\t42 : Ticket_item [CATEGORICAL]\n",
    "\t31 : Sex [CATEGORICAL]\n",
    "\t28 : Parch [NUMERICAL]\n",
    "\t22 : Ticket_number [CATEGORICAL]\n",
    "\t15 : Pclass [NUMERICAL]\n",
    "\t12 : Embarked [CATEGORICAL]\n",
    "\t5 : SibSp [NUMERICAL]\n",
    "\n",
    "Condition type in nodes:\n",
    "\t744 : ObliqueCondition\n",
    "\t122 : ContainsBitmapCondition\n",
    "\t29 : ContainsCondition\n",
    "Condition type in nodes with depth <= 0:\n",
    "\t31 : ContainsBitmapCondition\n",
    "\t2 : ContainsCondition\n",
    "Condition type in nodes with depth <= 1:\n",
    "\t64 : ObliqueCondition\n",
    "\t33 : ContainsBitmapCondition\n",
    "\t2 : ContainsCondition\n",
    "Condition type in nodes with depth <= 2:\n",
    "\t176 : ObliqueCondition\n",
    "\t51 : ContainsBitmapCondition\n",
    "\t3 : ContainsCondition\n",
    "Condition type in nodes with depth <= 3:\n",
    "\t380 : ObliqueCondition\n",
    "\t77 : ContainsBitmapCondition\n",
    "\t18 : ContainsCondition\n",
    "Condition type in nodes with depth <= 5:\n",
    "\t744 : ObliqueCondition\n",
    "\t122 : ContainsBitmapCondition\n",
    "\t29 : ContainsCondition\n",
    "\n",
    "Training logs:\n",
    "Number of iteration to final model: 33\n",
    "\tIter:1 train-loss:1.266350 valid-loss:1.360049  train-accuracy:0.624531 valid-accuracy:0.543478\n",
    "\tIter:2 train-loss:1.213702 valid-loss:1.321897  train-accuracy:0.624531 valid-accuracy:0.543478\n",
    "\tIter:3 train-loss:1.165783 valid-loss:1.286817  train-accuracy:0.624531 valid-accuracy:0.543478\n",
    "\tIter:4 train-loss:1.122469 valid-loss:1.256133  train-accuracy:0.624531 valid-accuracy:0.543478\n",
    "\tIter:5 train-loss:1.081461 valid-loss:1.229342  train-accuracy:0.808511 valid-accuracy:0.771739\n",
    "\tIter:6 train-loss:1.045305 valid-loss:1.204601  train-accuracy:0.826033 valid-accuracy:0.728261\n",
    "\tIter:16 train-loss:0.794952 valid-loss:1.058568  train-accuracy:0.914894 valid-accuracy:0.771739\n",
    "\tIter:26 train-loss:0.646146 valid-loss:1.021539  train-accuracy:0.926158 valid-accuracy:0.793478\n",
    "\tIter:36 train-loss:0.558627 valid-loss:1.023663  train-accuracy:0.929912 valid-accuracy:0.771739\n",
    "\tIter:46 train-loss:0.493899 valid-loss:1.025164  train-accuracy:0.931164 valid-accuracy:0.760870\n",
    "\tIter:56 train-loss:0.451528 valid-loss:1.032880  train-accuracy:0.938673 valid-accuracy:0.771739"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ï¼gptï¼Here's a simple breakdown of the Keras model summary:\n",
    "\n",
    "##### Model Type & Task:\n",
    "- **Model:** Gradient Boosted Trees Model (like XGBoost or similar).\n",
    "- **Task:** Classification problem (predicting categories).\n",
    "- **Label:** The target label is called `__LABEL`.\n",
    "\n",
    "##### Input Features:\n",
    "- **11 Features:** These are the columns or variables used by the model for prediction, like `Age`, `Cabin`, `Sex`, `Ticket_number`, etc.\n",
    "\n",
    "##### Variable Importance:\n",
    "- **INV_MEAN_MIN_DEPTH:** Shows which features are most important based on their depth in the trees.\n",
    "- **NUM_AS_ROOT:** Shows how many times each feature is used as the \"root\" feature at the top of the trees.\n",
    "- **NUM_NODES:** Counts how often a feature is used across the total number of nodes in all trees.\n",
    "- **SUM_SCORE:** Measures the overall contribution of each feature to the modelâs predictions.\n",
    "\n",
    "##### Model Loss & Performance:\n",
    "- **Loss Function:** The model uses **Binomial Log Likelihood**, which is common for binary classification tasks.\n",
    "- **Validation Loss:** The model's error (1.01542) on the validation data, lower values are better.\n",
    "- **Number of Trees:** 33 trees in total, where each tree helps make a decision.\n",
    "- **Total Nodes:** There are 1,823 decision points in the trees.\n",
    "\n",
    "##### Tree Structure:\n",
    "- **Number of Nodes per Tree:** There are 33 trees, and each tree has around 55 nodes on average.\n",
    "- **Depth by Leafs:** Most leaves in the trees have depth 5 (indicating the final decision-making level).\n",
    "- **Training Observations per Leaf:** The number of training examples (data points) handled by each leaf.\n",
    "\n",
    "##### Attributes Used in Nodes:\n",
    "- **Age, Sex, Fare, Name, etc.:** These are the attributes/features involved in the decision-making at different levels of the tree.\n",
    "- **Attributes with Depth <= 0:** Features like `Sex` and `Name` appear early (shallow) in the decision trees.\n",
    "- **Condition Types:** Oblique conditions are used to make complex splits in the data at the decision nodes.\n",
    "\n",
    "##### Training Logs:\n",
    "- **Iterations:** The model underwent 33 iterations to improve, with the training loss decreasing over time and the accuracy increasing.\n",
    "- **Final Model Accuracy:** It achieved around 92.9% accuracy on the training data at the end.\n",
    "\n",
    "In summary, this model is a Gradient Boosted Trees classifier that uses several features to predict categories, with `Sex`, `Age`, and `Fare` being the most influential. The model is progressively improved through multiple iterations, and it has good training performance (accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def prediction_to_kaggle_format(model, threshold=0.5):\n",
    "    proba_survive = model.predict(serving_ds, verbose=0)[:,0]\n",
    "    return pd.DataFrame({\n",
    "        \"PassengerId\": serving_df[\"PassengerId\"],\n",
    "        \"Survived\": (proba_survive >= threshold).astype(int)\n",
    "    })\n",
    "\n",
    "def make_submission(kaggle_predictions):\n",
    "    path=\"/kaggle/working/submission.csv\"\n",
    "    kaggle_predictions.to_csv(path, index=False)\n",
    "    print(f\"Submission exported to {path}\")\n",
    "    \n",
    "kaggle_predictions = prediction_to_kaggle_format(model)\n",
    "make_submission(kaggle_predictions)\n",
    "!head /kaggle/working/submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Submission exported to /kaggle/working/submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final accuracy (submitted):**0.80143**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model with hyperparameter tuning\n",
    "Hyper-parameter tuning is enabled by specifying the tuner constructor argument of the model. The tuner object contains all the configuration of the tuner (search space, optimizer, trial and objective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tuner = tfdf.tuner.RandomSearch(num_trials=1000)\n",
    "tuner.choice(\"min_examples\", [2, 5, 7, 10])\n",
    "tuner.choice(\"categorical_algorithm\", [\"CART\", \"RANDOM\"])\n",
    "\n",
    "local_search_space = tuner.choice(\"growing_strategy\", [\"LOCAL\"])\n",
    "local_search_space.choice(\"max_depth\", [3, 4, 5, 6, 8])\n",
    "\n",
    "global_search_space = tuner.choice(\"growing_strategy\", [\"BEST_FIRST_GLOBAL\"], merge=True)\n",
    "global_search_space.choice(\"max_num_nodes\", [16, 32, 64, 128, 256])\n",
    "\n",
    "#tuner.choice(\"use_hessian_gain\", [True, False])\n",
    "tuner.choice(\"shrinkage\", [0.02, 0.05, 0.10, 0.15])\n",
    "tuner.choice(\"num_candidate_attributes_ratio\", [0.2, 0.5, 0.9, 1.0])\n",
    "\n",
    "\n",
    "tuner.choice(\"split_axis\", [\"AXIS_ALIGNED\"])\n",
    "oblique_space = tuner.choice(\"split_axis\", [\"SPARSE_OBLIQUE\"], merge=True)\n",
    "oblique_space.choice(\"sparse_oblique_normalization\",\n",
    "                     [\"NONE\", \"STANDARD_DEVIATION\", \"MIN_MAX\"])\n",
    "oblique_space.choice(\"sparse_oblique_weights\", [\"BINARY\", \"CONTINUOUS\"])\n",
    "oblique_space.choice(\"sparse_oblique_num_projections_exponent\", [1.0, 1.5])\n",
    "\n",
    "# Tune the model. Notice the `tuner=tuner`.\n",
    "tuned_model = tfdf.keras.GradientBoostedTreesModel(tuner=tuner)\n",
    "tuned_model.fit(train_ds, verbose=0)\n",
    "\n",
    "tuned_self_evaluation = tuned_model.make_inspector().evaluation()\n",
    "print(f\"Accuracy: {tuned_self_evaluation.accuracy} Loss:{tuned_self_evaluation.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Accuracy: 0.9178082346916199 Loss:0.6503586769104004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "kaggle_predictions = prediction_to_kaggle_format(tuned_model)\n",
    "make_submission(kaggle_predictions)\n",
    "!head /kaggle/working/submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information you can follow this tutorial: [Automated hyper-parameter tuning](https://www.tensorflow.org/decision_forests/tutorials/automatic_tuning_colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final result: **0.80143**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making an ensemble\n",
    "Here you'll create 100 models with different seeds and combine their results.\n",
    "- This approach removes a little bit the random aspects related to creating ML models\n",
    "- In the GBT creation is used the honest parameter. It will use different training examples to infer the structure and the leaf values. This regularization technique trades examples for bias estimatesã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "predictions = None\n",
    "num_predictions = 0\n",
    "\n",
    "for i in range(100):\n",
    "    print(f\"i:{i}\")\n",
    "    # Possible models: GradientBoostedTreesModel or RandomForestModel\n",
    "    model = tfdf.keras.GradientBoostedTreesModel(\n",
    "        verbose=0, # Very few logs\n",
    "        features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n",
    "        exclude_non_specified_features=True, # Only use the features in \"features\"\n",
    "\n",
    "        #min_examples=1,\n",
    "        #categorical_algorithm=\"RANDOM\",\n",
    "        ##max_depth=4,\n",
    "        #shrinkage=0.05,\n",
    "        ##num_candidate_attributes_ratio=0.2,\n",
    "        #split_axis=\"SPARSE_OBLIQUE\",\n",
    "        #sparse_oblique_normalization=\"MIN_MAX\",\n",
    "        #sparse_oblique_num_projections_exponent=2.0,\n",
    "        #num_trees=2000,\n",
    "        ##validation_ratio=0.0,\n",
    "        random_seed=i,\n",
    "        honest=True,\n",
    "    )\n",
    "    model.fit(train_ds)\n",
    "    \n",
    "    sub_predictions = model.predict(serving_ds, verbose=0)[:,0]\n",
    "    if predictions is None:\n",
    "        predictions = sub_predictions\n",
    "    else:\n",
    "        predictions += sub_predictions\n",
    "    num_predictions += 1\n",
    "\n",
    "predictions/=num_predictions\n",
    "\n",
    "kaggle_predictions = pd.DataFrame({\n",
    "        \"PassengerId\": serving_df[\"PassengerId\"],\n",
    "        \"Survived\": (predictions >= 0.5).astype(int)\n",
    "    })\n",
    "\n",
    "make_submission(kaggle_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final result: **0.80143**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(GPT)\n",
    "In simple terms, this section is explaining how to **create an ensemble of models** to improve the accuracy and stability of the predictions:\n",
    "\n",
    "1. **Making an Ensemble of 100 Models:**\n",
    "   - The idea is to create **100 different models** with different **random seeds** (which are like starting points for the model's training process).\n",
    "   - By using different seeds, each model will be slightly different, and this diversity helps to improve the overall performance when you combine their results. This is called an **ensemble**, where you take multiple models and combine their outputs to make a final prediction. It can reduce errors that happen because of randomness in how a single model might learn from the data.\n",
    "\n",
    "2. **Removing Randomness:**\n",
    "   - Normally, machine learning models can have some randomness in how they are trained, which can lead to different results each time you train a model (even on the same data).\n",
    "   - By creating many models with different random seeds and combining their results, you **reduce the effect of randomness**. This helps make the final prediction more reliable and consistent.\n",
    "\n",
    "3. **\"Honest\" Parameter in Gradient Boosted Trees (GBT):**\n",
    "   - The **\"honest\" parameter** is a special setting in the modelâs creation. It helps the model by using **different examples** (data points) to **learn the structure of the model** (how the tree splits at each point) and to **estimate the values** (what the predicted output should be for each branch of the tree).\n",
    "   - This helps reduce the risk of the model **overfitting** the data (getting too specific to the training data and not generalizing well to new data).\n",
    "   - The idea is that by trading off a bit of **bias (slightly less accurate predictions on the training set)** for **more stable estimates** (better predictions on new data), the model becomes more **robust** and better at making predictions.\n",
    "\n",
    "### Summary:\n",
    "- **Ensemble of 100 models**: This creates multiple models with different seeds, and then combines their predictions to make the final output more reliable.\n",
    "- **Honest parameter**: This regularization technique makes the model more general by using different training examples for learning the structure and values of the tree. It helps reduce overfitting and makes the model better at generalizing to new data.\n",
    "\n",
    "In short, itâs a way to make the model **stronger** and **more stable** by reducing randomness and improving how the model generalizes from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (gpt) Further to try with\n",
    "If using an ensemble of 100 models in **Gusthema's approach** still results in a model accuracy of about **80%** and you're looking to improve it further, there are a few strategies and improvements you could try. Iâll break them down into possible steps for you:\n",
    "\n",
    "### 1. **Hyperparameter Tuning for TF-DT**\n",
    "   - **Parameter tuning** is one of the most effective ways to improve model performance. While you mentioned using ensemble models, ensuring you're using **optimal hyperparameters** for **TensorFlow Decision Trees (TF-DT)** is critical. \n",
    "   - Parameters you can tune:\n",
    "     - **Learning rate**: A higher learning rate can make the model converge faster but may miss the optimal solution. A lower learning rate might improve precision but slow down training.\n",
    "     - **Number of trees**: You can try increasing the number of trees in the ensemble (instead of 100, try 150 or 200).\n",
    "     - **Maximum depth of trees**: Deeper trees might capture more complex patterns but also risk overfitting. Try experimenting with different depths.\n",
    "     - **Leaf size**: The number of data points each leaf node in the decision tree contains can impact generalization.\n",
    "   - Use **GridSearch** or **RandomizedSearch** to try different combinations of these parameters.\n",
    "\n",
    "### 2. **Advanced Ensembling Techniques**\n",
    "   - Instead of just averaging the predictions of 100 models, you can use more sophisticated ensembling techniques:\n",
    "     - **Stacking**: Train a second-level model that combines the predictions from the individual models to make a final prediction. This can often improve accuracy by leveraging the strengths of individual models.\n",
    "     - **Weighted Averaging**: Instead of giving each model equal weight, give more importance to models that perform better on a validation set.\n",
    "     - **Bagging**: Bagging (Bootstrap Aggregating) can help by creating different subsets of your training data and training each model on a different subset. This often reduces variance and improves model generalization.\n",
    "\n",
    "### 3. **Feature Engineering**\n",
    "   - **Feature scaling**: Decision trees are usually insensitive to feature scaling, but other models in your ensemble might benefit from it. Scaling or normalizing features could help some algorithms like **Logistic Regression** or **SVMs** that might be part of your ensemble.\n",
    "   - **Feature selection**: You could try reducing the feature space by selecting only the most relevant features for training. Sometimes, less is more, and trimming irrelevant or noisy features can help boost accuracy.\n",
    "   - **Creating new features**: You could try creating interaction terms or higher-order features (e.g., combining **Age** and **Fare**) to give the model more information.\n",
    "\n",
    "### 4. **Try Other Models in the Ensemble**\n",
    "   - **Include different types of models**: While TF-DT is a great model, sometimes combining it with other models can improve accuracy. Consider adding models like **XGBoost**, **Random Forests**, or **Logistic Regression** as part of your ensemble.\n",
    "   - **Gradient Boosting**: Instead of using just TensorFlow Decision Trees, you could include **Gradient Boosting Models (GBM)** or even **XGBoost**. These models perform well on tabular data and might capture patterns that TF-DT misses.\n",
    "\n",
    "### 5. **Cross-validation**\n",
    "   - **K-fold cross-validation**: Instead of using a single training-validation split, use **K-fold cross-validation** to make sure your model is evaluated across multiple splits. This will give you a better sense of model performance and reduce overfitting.\n",
    "   - **Stratified K-folds**: For classification problems, ensure you're using **Stratified K-fold cross-validation**, which ensures the distribution of the target variable is the same in each fold, preventing biased validation results.\n",
    "\n",
    "### 6. **Early Stopping / Model Checkpoints**\n",
    "   - Use **early stopping** to prevent overfitting and allow the model to stop training once the performance on the validation set stops improving. This is useful when training a large number of trees and ensures you're not overtraining.\n",
    "   - If you're already using early stopping, make sure your **patience** parameter is set appropriately to avoid prematurely stopping training before the model has learned enough.\n",
    "\n",
    "### 7. **Data Augmentation / More Data**\n",
    "   - If your model is still not reaching the desired accuracy, consider if there are ways to **increase the amount of data**.\n",
    "     - **Synthetic data generation**: If you donât have more data, you could try **synthetic data augmentation** techniques or **bootstrapping** (sampling with replacement) to increase the variety of data your models are trained on.\n",
    "     - **Feature engineering** to create new data points, or transforming existing ones into more useful formats, can help in some cases.\n",
    "\n",
    "### 8. **Model Diagnostics**\n",
    "   - **Analyze model errors**: Look at **misclassified instances** and see if you can identify patterns in the errors. It might help you fine-tune your feature engineering or even spot outliers that are negatively affecting the model.\n",
    "   - **Shapley Values** or **Feature Importance**: Use tools like **SHAP** or **LIME** to analyze which features are important for the model's predictions and if they can be improved.\n",
    "\n",
    "### 9. **Hyperparameter Tuning on Multiple Models in the Ensemble**\n",
    "   - Instead of just tuning parameters for one model in the ensemble, consider **hyperparameter optimization for each individual model** in the ensemble. If you're using a variety of models, you can apply **Bayesian Optimization** or **RandomizedSearch** for each model in the ensemble separately.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Suggestions:\n",
    "| **Improvement Area**                  | **Suggestions**                                                                                  |\n",
    "|---------------------------------------|--------------------------------------------------------------------------------------------------|\n",
    "| **Hyperparameter Tuning**             | Tune hyperparameters like learning rate, number of trees, depth, and leaf size. Use grid or random search. |\n",
    "| **Ensemble Techniques**               | Try stacking, weighted averaging, or bagging for better results.                                  |\n",
    "| **Feature Engineering**               | Normalize features, reduce noise, or create new meaningful features (e.g., Age + Fare interaction).|\n",
    "| **Model Variety in Ensemble**        | Include models like XGBoost, Random Forest, or even Logistic Regression alongside TF-DT.          |\n",
    "| **Cross-Validation**                  | Use K-fold cross-validation (Stratified) to reduce overfitting and improve model evaluation.     |\n",
    "| **Early Stopping**                    | Implement early stopping with appropriate patience to avoid overtraining.                        |\n",
    "| **More Data / Data Augmentation**    | Increase the data size through augmentation or synthetic data generation.                        |\n",
    "| **Model Diagnostics**                 | Analyze misclassifications and use SHAP/LIME to understand feature importance.                   |\n",
    "| **Hyperparameter Tuning on Ensemble** | Perform hyperparameter tuning separately on each individual model in the ensemble.               |\n",
    "\n",
    "By implementing some of these techniques, especially tuning hyperparameters, diversifying the ensemble, and using advanced ensembling methods, you might see an improvement in your model accuracy."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
